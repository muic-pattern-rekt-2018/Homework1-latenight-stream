{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. Mushroom Weight and Height\n",
    "\n",
    "In the class we build a naive bayes classifier which classify whether a mushroom is poisonous or edible. In the class, all the feature are of categorical type; it has a discrete number of outcome as opposed to real number.\n",
    "\n",
    "In this problem we want to explore two ways to deal with real number features for Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "The data given to you is very similar to mushroom data. It has three features: cap-color(with the same dictionary we did in class), mushroom-weight(made up by me), mushroom-height(also made up by me).\n",
    "\n",
    "The data is given in\n",
    "`mushrooms_homework_test.csv`\n",
    "and\n",
    "`mushrooms_homework_train.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1) Simplest way. Just bin it.\n",
    "\n",
    "The simplest way for dealing with continuous value feature is to discretize it. The simplest way to discretize is just to bin it. For example if your data looks like\n",
    "\n",
    "`data = (0.9, 1.1, 1.2, 2.1, 2.2, 4.2, 5.3, 5.1)`\n",
    "\n",
    "We count bin it with bin edges of \n",
    "\n",
    "`bins = (1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)`\n",
    "\n",
    "the bin number of a data point $x$ is the index $i$ where `bins[i-1] < x < bins[i]`. If $x$ is less than the minimum of bin edge then the bin number is 0. If $x$ is more than the maximum of bin edge then it's `len(bins)`.\n",
    "\n",
    "Ex the data points above would be turned into\n",
    "`binno = (0, 1, 1, 3, 3, 7, 9, 9)`\n",
    "\n",
    "Once we discretize the value we can just use Bayes Classifier we did in class.\n",
    "\n",
    "**Your task is to build a naive bayes classifier with binned height and binned weight. Pick appropriate bin edges somehow.\n",
    "Then test your algorithm on the test data set. Report how many you got right and wrong in test data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xclass</th>\n",
       "      <th>cap_color</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>y</td>\n",
       "      <td>6.122458</td>\n",
       "      <td>7.143689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>w</td>\n",
       "      <td>4.709259</td>\n",
       "      <td>7.398728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>2.341551</td>\n",
       "      <td>4.733059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>g</td>\n",
       "      <td>3.954025</td>\n",
       "      <td>4.040427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>y</td>\n",
       "      <td>3.456429</td>\n",
       "      <td>6.422466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6502</th>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>3.103014</td>\n",
       "      <td>3.211495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>5.162033</td>\n",
       "      <td>6.841161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>4.754516</td>\n",
       "      <td>5.347342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>3.272145</td>\n",
       "      <td>8.031833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>4.199607</td>\n",
       "      <td>4.250340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6507 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     xclass cap_color    weight    height\n",
       "0         e         y  6.122458  7.143689\n",
       "1         e         w  4.709259  7.398728\n",
       "2         p         w  2.341551  4.733059\n",
       "3         e         g  3.954025  4.040427\n",
       "4         e         y  3.456429  6.422466\n",
       "...     ...       ...       ...       ...\n",
       "6502      p         n  3.103014  3.211495\n",
       "6503      e         n  5.162033  6.841161\n",
       "6504      e         n  4.754516  5.347342\n",
       "6505      e         n  3.272145  8.031833\n",
       "6506      e         n  4.199607  4.250340\n",
       "\n",
       "[6507 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/mushrooms_homework_train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bin_column(col_data):\n",
    "    bins = np.linspace(np.min(col_data), np.max(col_data), 20)\n",
    "    return np.digitize(col_data, bins=bins)\n",
    "\n",
    "df['dheight'] = make_bin_column(df.height)\n",
    "df['dweight'] = make_bin_column(df.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['cap_color', 'dweight', 'dheight']]\n",
    "xclasses = df['xclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def max_value_key(d: Dict[str, float])->str:\n",
    "    best_key = None\n",
    "    best_value = 0.\n",
    "    first = True\n",
    "    for k, v in d.items():\n",
    "        if first or v>best_value:\n",
    "            best_key = k\n",
    "            best_value = v\n",
    "            first=False\n",
    "    return best_key\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProbKey:\n",
    "    fname: str\n",
    "    value: str\n",
    "    cls: str\n",
    "    \n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        # self.prob_dict (feature_name, value, classname) -> \n",
    "        self.prob_dict = {}\n",
    "        self.fnames = []\n",
    "        self.all_classes = []\n",
    "        self.prior = {}\n",
    "        \n",
    "    def _cal_prob(self, features, xclasses, fname, value, cls):\n",
    "        value_mask = features[fname] == value\n",
    "        cls_mask = xclasses == cls\n",
    "        n_right = np.sum(value_mask & cls_mask)\n",
    "        n_cls = sum(cls_mask)\n",
    "        prob = n_right / n_cls\n",
    "        return prob \n",
    "        \n",
    "    \n",
    "    def train(self, features, xclasses):\n",
    "        all_classes = xclasses.unique()\n",
    "        self.fnames = features.columns\n",
    "        self.all_classes = all_classes\n",
    "        prob_dict = {}\n",
    "        self.prior = {cls:sum(xclasses==cls)/len(xclasses) for cls in all_classes}\n",
    "        \n",
    "        for fname in features.columns:\n",
    "            for value in features[fname].unique():\n",
    "                for cls in all_classes:\n",
    "                    key = ProbKey(fname=fname, value=value, cls=cls)\n",
    "                    prob = self._cal_prob(features, xclasses, fname, value, cls)\n",
    "                    prob_dict[key] = prob\n",
    "        self.prob_dict = prob_dict\n",
    "    \n",
    "    def predict_top_one(self, data, cls: str):\n",
    "        p = self.prior[cls]\n",
    "        for fname in self.fnames:\n",
    "            value = getattr(data, fname)\n",
    "            key = ProbKey(fname, value, cls)\n",
    "            this_p = self.prob_dict[key]\n",
    "            p *= this_p\n",
    "        return p #just the prior*Prod P(x|cls)\n",
    "            \n",
    "    def predict_prob(self, data):\n",
    "        top_dict = {cls: self.predict_top_one(data, cls) for cls in self.all_classes}\n",
    "        evidence = sum(v for k,v in top_dict.items())\n",
    "        return {cls: v/evidence for cls, v in top_dict.items()}\n",
    "    \n",
    "    def predict_class(self, data):\n",
    "        #return cls with highest prob\n",
    "        probs = self.predict_prob(data)\n",
    "        best_cls = max_value_key(probs)\n",
    "        return best_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.train(features, xclasses)\n",
    "# for data, xclass in zip(features.itertuples(), xclasses):\n",
    "#     print(nb.predict_prob(data), xclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/mushrooms_homework_test.csv')\n",
    "test_df['dheight'] = make_bin_column(test_df.height)\n",
    "test_df['dweight'] = make_bin_column(test_df.weight)\n",
    "\n",
    "test_features = test_df[['cap_color', 'dweight', 'dheight']]\n",
    "test_xclasses = test_df['xclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4771 6507 0.7332103888120486\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data, xclass in zip(features.itertuples(), xclasses):\n",
    "    if nb.predict_class(data) == xclass:\n",
    "        correct+=1\n",
    "    total += 1\n",
    "print(correct, total, correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2) Gaussian Naive Bayes.\n",
    "\n",
    "We could assume a certain probability distribution function(pdf) for the conditional probability. One popular choice is normal distrubution/gaussian distribution.\n",
    "\n",
    "Let us understand this assumption intuitively. The idea is that the *weight* of *poisonous* mushroom is normally distributed around some mean with some width while the *weight* for edible mushroom is hopefully normally distributed around some other mean.\n",
    "\n",
    "Let us say that the weight of poisonous mushroom is normally distributed around $2\\pm1$ gram(I made up this number)  while the edible mushroom is normally distributed at $5\\pm 1$ gram. If we found a mushroom of 2.5 gram. It's probably the poisonous one since edible one should weight around 5 gram.\n",
    "\n",
    "<img src=\"gaussian.png\" width=\"400\"/>\n",
    "\n",
    "Concretely, we assume that the probability distribution of feature $X$ given that it is of class $y$ to be\n",
    "\n",
    "$$\n",
    "\\displaystyle\n",
    "pdf(X=x|y) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left[\\frac{-(x-\\mu_y)^2}{2\\sigma_y^2}\\right]\n",
    "$$\n",
    "\n",
    " - $\\mu_y$ is the mean of feature $X$ given that it is of class $y$. Ex: mean weight($X$) of poisonous mushroom (eg: 2 gram).\n",
    "\n",
    " - $\\sigma_y$ is the standard deviation of feature $X$ given that it is of class $y$. Ex std. dev. of weight of poisonous mushroom(eg: \\pm 1 gram)\n",
    " \n",
    "Recall the relatio between pdf and probability.\n",
    "$$\n",
    "    P(X \\in (x, x+\\delta x)|y) = \\int^{x+\\delta x}_x pdf(X=x) \\;\\text{d}x\n",
    "$$\n",
    "\n",
    "for small enough $\\delta x$ it becomes\n",
    "$$\n",
    "    P(X \\in (x, x+\\delta x)|y) = pdf(X=x) \\delta x\n",
    "$$\n",
    "\n",
    "Now here is the important part. From the above $P(X=x|y)$ and $P(X=x|\\sim y)$ has a factor of $\\delta x$(I drop of the range for brevity). This means that the factor of $\\delta x$ appear in both the numerator and denominator of the formula we use for calculating probability. Thus the $\\delta x$ cancels out nicely. This means that **we can just use pdf(X=x|y) as P(X=x|y)** in naive bayes formula we got and every thing will just work out. We don't have to worry about the $\\delta x$ part\n",
    "\n",
    "**Your task: build a classifier similar to what you did in 1.1. Except now you use gaussian distribution assumption for the continous features. Measure your performance against the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def gaussian(x, mu, sigma):\n",
    "    return 1/np.sqrt(2*np.pi)*np.exp(-(x-mu)**2/2/sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class GaussianParam:\n",
    "    mu: float\n",
    "    sigma: float\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GaussianKey:\n",
    "    fname: str\n",
    "    cls: str\n",
    "    \n",
    "class NaiveBayesGaussian:\n",
    "    def __init__(self):\n",
    "        # self.prob_dict (feature_name, value, classname) ->\n",
    "        self.prob_dict = {}\n",
    "        self.all_classes = []\n",
    "        self.prior = {}\n",
    "        self.categorical_features = []  # list of categorical features\n",
    "        self.continuous_features = []  # list of continuous features\n",
    "        self.gaussian_param = {}  # dict from continuous featurename -> Gaussian param\n",
    "\n",
    "    def _cal_prob(self, features, xclasses, fname, value, cls):\n",
    "        value_mask = features[fname] == value\n",
    "        cls_mask = xclasses == cls\n",
    "        n_right = np.sum(value_mask & cls_mask)\n",
    "        n_cls = sum(cls_mask)\n",
    "        prob = n_right / n_cls\n",
    "        return prob\n",
    "\n",
    "    def train(self, features, xclasses, continuous_features: List[str] = None):\n",
    "        if continuous_features is None:\n",
    "            continuous_features = []\n",
    "\n",
    "        self.categorical_features = [\n",
    "            col for col in features.columns if col not in continuous_features]\n",
    "        self.continuous_features = continuous_features\n",
    "        all_classes = xclasses.unique()\n",
    "        self.all_classes = all_classes\n",
    "        prob_dict = {}\n",
    "        self.prior = {cls: sum(xclasses == cls)/len(xclasses)\n",
    "                      for cls in all_classes}\n",
    "\n",
    "        for fname in self.categorical_features:\n",
    "            for value in features[fname].unique():\n",
    "                for cls in all_classes:\n",
    "                    key = ProbKey(fname=fname, value=value, cls=cls)\n",
    "                    prob = self._cal_prob(\n",
    "                        features, xclasses, fname, value, cls)\n",
    "                    prob_dict[key] = prob\n",
    "        self.prob_dict = prob_dict\n",
    "\n",
    "        # for continuous one we use the gaussian\n",
    "        def get_gaussian_features(col, cls):\n",
    "            feature = features[col][xclasses==cls]\n",
    "            return GaussianParam(mu=np.mean(feature), sigma=np.std(feature))\n",
    "        self.gaussian_param = {GaussianKey(fname=col, cls=cls):get_gaussian_features(col, cls) \n",
    "                               for col in self.continuous_features\n",
    "                               for cls in self.all_classes}\n",
    "        \n",
    "\n",
    "    def predict_top_one(self, data, cls: str):\n",
    "        p = self.prior[cls]\n",
    "        for fname in self.categorical_features:\n",
    "            value = getattr(data, fname)\n",
    "            key = ProbKey(fname, value, cls)\n",
    "            this_p = self.prob_dict[key]\n",
    "            p *= this_p\n",
    "        \n",
    "        for fname in self.continuous_features:\n",
    "            gp = self.gaussian_param[GaussianKey(fname=fname, cls=cls)]\n",
    "            value = getattr(data, fname)\n",
    "            p *= gaussian(value, gp.mu, gp.sigma)\n",
    "        return p  # just the prior*Prod P(x|cls)\n",
    "\n",
    "    def predict_prob(self, data):\n",
    "        top_dict = {cls: self.predict_top_one(\n",
    "            data, cls) for cls in self.all_classes}\n",
    "        evidence = sum(v for k, v in top_dict.items())\n",
    "        return {cls: v/evidence for cls, v in top_dict.items()}\n",
    "\n",
    "    def predict_class(self, data):\n",
    "        # return cls with highest prob\n",
    "        probs = self.predict_prob(data)\n",
    "        best_cls = max_value_key(probs)\n",
    "        return best_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xclass</th>\n",
       "      <th>cap_color</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>y</td>\n",
       "      <td>6.122458</td>\n",
       "      <td>7.143689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>w</td>\n",
       "      <td>4.709259</td>\n",
       "      <td>7.398728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>2.341551</td>\n",
       "      <td>4.733059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>g</td>\n",
       "      <td>3.954025</td>\n",
       "      <td>4.040427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>y</td>\n",
       "      <td>3.456429</td>\n",
       "      <td>6.422466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6502</th>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>3.103014</td>\n",
       "      <td>3.211495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>5.162033</td>\n",
       "      <td>6.841161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>4.754516</td>\n",
       "      <td>5.347342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>3.272145</td>\n",
       "      <td>8.031833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>4.199607</td>\n",
       "      <td>4.250340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6507 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     xclass cap_color    weight    height\n",
       "0         e         y  6.122458  7.143689\n",
       "1         e         w  4.709259  7.398728\n",
       "2         p         w  2.341551  4.733059\n",
       "3         e         g  3.954025  4.040427\n",
       "4         e         y  3.456429  6.422466\n",
       "...     ...       ...       ...       ...\n",
       "6502      p         n  3.103014  3.211495\n",
       "6503      e         n  5.162033  6.841161\n",
       "6504      e         n  4.754516  5.347342\n",
       "6505      e         n  3.272145  8.031833\n",
       "6506      e         n  4.199607  4.250340\n",
       "\n",
       "[6507 rows x 4 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/mushrooms_homework_train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['cap_color', 'weight', 'height']]\n",
    "xclasses = df.xclass\n",
    "nb = NaiveBayesGaussian()\n",
    "nb.train(features, xclasses, continuous_features = ['weight', 'height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{GaussianKey(fname='weight', cls='e'): GaussianParam(mu=4.310016666234858, sigma=1.2925952634849398),\n",
       " GaussianKey(fname='weight', cls='p'): GaussianParam(mu=3.2141575420054664, sigma=0.9972401361343372),\n",
       " GaussianKey(fname='height', cls='e'): GaussianParam(mu=5.995639636920748, sigma=1.9003781732257048),\n",
       " GaussianKey(fname='height', cls='p'): GaussianParam(mu=5.00334707527546, sigma=1.4240001555328947)}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.gaussian_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2141575420054664"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.weight[df.xclass=='p'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4691 6507 0.7209159366835716\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data, xclass in zip(features.itertuples(), xclasses):\n",
    "    if nb.predict_class(data) == xclass:\n",
    "        correct+=1\n",
    "    total += 1\n",
    "print(correct, total, correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Product Reviews\n",
    "\n",
    "Naive Bayes is quite powerful given its simplicity. Typically the usefulness of a Machine learning Algorithm is limited only by your imgination on what to ask. If you ask an interesting question, you got a useful system. If you ask a dump question, you got a useless system.\n",
    "\n",
    "In this problem we will explore a text mining application using Naive Bayes.\n",
    "\n",
    "The goal of this problem is to make a system that can read customer review and tells whether the customer recommend the product to others or not.\n",
    "\n",
    "The data is stolen from https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews\n",
    "I splitted it into train(`clothing_reviews_train.csv`) and test(`clothing_reviews_test.csv`) for you.\n",
    "\n",
    "The two columns that is relavant to this problem are.\n",
    "- Recommended IND\n",
    "- Title\n",
    "- Review Text\n",
    "\n",
    "**Do not use any other column**\n",
    "\n",
    "You could do challenging version (No extra point except for bragging rights)\n",
    "Use the data from http://jmcauley.ucsd.edu/data/amazon/ and do similar thing --> the book review is hugeeeeee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task\n",
    "Build a classifier which you can give it a reviewtext and review title and it can split out whether the reviewer recommend the product or not. Measure the performance on test data `clothing_reviews_test.csv`.\n",
    "\n",
    "## Some Guide for you.\n",
    "\n",
    "- Be sure to normalize your text. Example [here](https://machinelearningmastery.com/clean-text-machine-learning-python/). This includes\n",
    "    - lowercase everything\n",
    "    - clean out stop words\n",
    "    - get rid of punctuations\n",
    "    - stem the word\n",
    "    - Yes you may use nltk only for cleaning up part.\n",
    "- Be careful as you are multiplying a whole bunch of small numbers together. You are better off adding the log and exponentiate it back.\n",
    "- Read up lecture notes on spam filtering. Especially on how to deal with missing words. You can read up [old version of excercise 1 from sam](https://github.com/KongsakTi/PatternReg/tree/master/Exercise%201)\n",
    "- **Do not** get stuck on this alone. Find help/Consult your friends if you are stuck. Collaboration is allowed but you must understand what you send in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you are now on your own!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
